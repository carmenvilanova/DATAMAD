{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto DATAMAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Carga de datos de Idealista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar importamos las librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "## Esta librería no te servirá en este análisis, pero\n",
    "## es la librería básica para scrapear páginas más sencillas\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "## Esta sí. Con ella, sacaremos datos del HTML.\n",
    "\n",
    "import random\n",
    "\n",
    "## Para usar números aleatorios. Esta no era tan complicada.\n",
    "## La usaremos para no usar el mismo tiempo siempre al scrapear.\n",
    "\n",
    "import time\n",
    "\n",
    "## Con esta metemos el retardo a la máquina. Las máquinas van rápidas. Yo no.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## EL bread and butter del Data Science. Tablas y operaciones aritméticas.\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "## Esta es la manera para usar selenium normal, pero nosotros tenemos que parecer humanos.\n",
    "## Por eso, usamos la siguiente librería.\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "## La persona que desarrolló esto es un crack. Configuró todos los proxies y\n",
    "## demás (digo demás porque no se qué brujería ha usado) para ser indetectable.\n",
    "## Usaremos este browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos el explorador, en este caso utilizamos Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = uc.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección extraemos de la página de idealista deseada los ids de las viviendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 30 IDs from https://www.idealista.com/venta-viviendas/madrid-madrid/pagina-5.htm\n",
      "Extracted 30 IDs from https://www.idealista.com/venta-viviendas/madrid-madrid/pagina-3.htm\n",
      "Extracted 30 IDs from https://www.idealista.com/venta-viviendas/madrid-madrid/pagina-2.htm\n",
      "Extracted 30 IDs from https://www.idealista.com/venta-viviendas/madrid-madrid/pagina-4.htm\n",
      "Extracted 30 IDs from https://www.idealista.com/venta-viviendas/madrid-madrid/pagina-1.htm\n",
      "Total IDs collected: 149\n",
      "['104440887', '103666203', '104608513', '105392078', '106132709', '106420775', '106054242', '105453336', '104161105', '104952998', '104663736', '102911946', '105036254', '106264579', '105070649', '105843197', '105774021', '105976873', '105890453', '106091596', '104613472', '103839118', '105386768', '106137924', '103784576', '105075962', '98249348', '105107064', '106135117', '105345023', '106330563', '102711209', '98352583', '103610280', '106300829', '106093199', '106155964', '105155727', '94336938', '105962129', '106080965', '106211397', '104651722', '106158684', '105814599', '103632538', '96982668', '104252638', '105924592', '106454523', '106364066', '104596158', '105432214', '101792453', '106016508', '105510209', '105501144', '105505309', '106135852', '106170730', '105829686', '106473746', '105436920', '105451572', '105971135', '102482662', '106232915', '101964725', '104484357', '105521069', '102729947', '104496367', '105386293', '105725481', '104830815', '104499449', '106167991', '105639958', '105457544', '104922553', '106407011', '97995920', '105105053', '105003804', '106337200', '105642015', '102043012', '105845137', '102750021', '105289066', '106313540', '106107690', '104154459', '105236114', '106324131', '106459458', '106139309', '106305775', '106395772', '105489630', '106091063', '105260875', '102248221', '106408678', '106144167', '105197920', '104254787', '103997327', '104544416', '104607160', '106017344', '105893822', '106199730', '105122568', '105889280', '106272590', '106213981', '105449412', '106473069', '103365843', '105168585', '106002365', '106151385', '105047027', '105767996', '105420010', '105820994', '105036875', '104302587', '105850524', '97875752', '104234563', '105497630', '104671284', '104515115', '105891810', '101183612', '105844720', '104841341', '106069307', '105897932', '106061403', '104281658', '103910133', '100335644', '104797646', '105426650', '106198314', '105413336']\n",
      "Exported 149 IDs to ids.csv\n",
      "DataFrame with extracted IDs:\n",
      "            ID\n",
      "0    104440887\n",
      "1    103666203\n",
      "2    104608513\n",
      "3    105392078\n",
      "4    106132709\n",
      "..         ...\n",
      "144  100335644\n",
      "145  104797646\n",
      "146  105426650\n",
      "147  106198314\n",
      "148  105413336\n",
      "\n",
      "[149 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import random\n",
    "import time\n",
    "import pandas as pd  # Para exportar los datos a CSV\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.by import By\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "# Configuración de Selenium con undetected_chromedriver\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--headless')  # No mostrar la ventana del navegador\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    # Crea el navegador sin ser detectado\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "# Función para extraer los IDs de una página de manera asíncrona\n",
    "async def fetch_page(session, url):\n",
    "    # Simulamos un navegador real con el User-Agent adecuado\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "\n",
    "    async with session.get(url, headers=headers) as response:\n",
    "        content = await response.text()\n",
    "        return url, response.status, content\n",
    "\n",
    "# Función para extraer los IDs de cada página HTML\n",
    "def extract_ids_from_html(html_content):\n",
    "    soup = bs(html_content, 'lxml')\n",
    "    try:\n",
    "        articles = soup.find('main', {'class': 'listing-items'}).find_all('article')\n",
    "        ids = [article.get('data-element-id') for article in articles if article.get('data-element-id') is not None]\n",
    "        return ids\n",
    "    except AttributeError:\n",
    "        return []\n",
    "\n",
    "# Función para procesar múltiples URLs en paralelo\n",
    "async def fetch_urls_in_parallel(url_list):\n",
    "    ids = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_page(session, url) for url in url_list]\n",
    "        for task in asyncio.as_completed(tasks):\n",
    "            url, status_code, content = await task\n",
    "            if status_code == 200:\n",
    "                page_ids = extract_ids_from_html(content)\n",
    "                ids.extend(page_ids)\n",
    "                print(f\"Extracted {len(page_ids)} IDs from {url}\")\n",
    "            else:\n",
    "                print(f\"Failed to fetch {url} with status code {status_code}\")\n",
    "    return ids\n",
    "\n",
    "# Función para manejar el scraping completo\n",
    "async def main_scraper():\n",
    "    # Empezamos con la creación del navegador (Selenium con undetected_chromedriver)\n",
    "    browser = get_driver()\n",
    "\n",
    "    # Lista de URLs a scrapear\n",
    "    x = 1\n",
    "    url_list = []\n",
    "\n",
    "    # Crear las URLs para las primeras 5 páginas\n",
    "    while True:\n",
    "        url = f'https://www.idealista.com/venta-viviendas/madrid-madrid/pagina-{x}.htm'\n",
    "        url_list.append(url)\n",
    "        x += 1\n",
    "        if x > 5:  # Limitar a las primeras 5 páginas para la prueba\n",
    "            break\n",
    "\n",
    "    # Usamos Selenium para aceptar cookies si es necesario\n",
    "    for url in url_list:\n",
    "        browser.get(url)\n",
    "        time.sleep(random.randint(10, 12))  # Retardo para parecer humano\n",
    "        try:\n",
    "            browser.find_element(By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]').click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Ahora, obtenemos las páginas de manera asíncrona\n",
    "    ids = await fetch_urls_in_parallel(url_list)\n",
    "\n",
    "    # Mostrar los resultados finales\n",
    "    ids = list(set(ids))  # Eliminar duplicados\n",
    "    print(f\"Total IDs collected: {len(ids)}\")\n",
    "    print(ids)\n",
    "\n",
    "    # Cerrar el navegador al final\n",
    "    browser.quit()\n",
    "\n",
    "    # Convertir los IDs en un DataFrame\n",
    "    df_ids = pd.DataFrame(ids, columns=[\"ID\"])\n",
    "\n",
    "    # Exportar los IDs a un archivo CSV\n",
    "    export_to_csv(df_ids)\n",
    "\n",
    "    # Devolver el DataFrame para usarlo en otras partes del código\n",
    "    return df_ids\n",
    "\n",
    "# Función para exportar los IDs a un archivo CSV\n",
    "def export_to_csv(df, filename=\"ids.csv\"):\n",
    "    df.to_csv(filename, index=False)  # Exportar a un CSV sin el índice\n",
    "    print(f\"Exported {len(df)} IDs to {filename}\")\n",
    "\n",
    "# Ejecutar el scraper en el bucle de eventos actual\n",
    "if __name__ == \"__main__\":\n",
    "    # Para Jupyter o IPython, no usamos asyncio.run(), simplemente usamos await\n",
    "    df_ids = await main_scraper()\n",
    "\n",
    "    # Ahora puedes usar df_ids en otras partes del código\n",
    "    print(\"DataFrame with extracted IDs:\")\n",
    "    print(df_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un data frame con los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID\n",
      "0  104440887\n",
      "1  103666203\n",
      "2  104608513\n",
      "3  105392078\n",
      "4  106132709\n"
     ]
    }
   ],
   "source": [
    "# Usando el DataFrame después de obtenerlo\n",
    "df_casas = df_ids  # Puedes usar directamente el DataFrame de IDs aquí\n",
    "\n",
    "# Hacer algo con el DataFrame\n",
    "print(df_casas.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y lo exportamos a un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "casas = pd.Series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, a partir del id de las casas, extraemos el resto de datos utilizando BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsear_inmueble(id_inmueble):\n",
    "    \n",
    "    print( '\\n Casa numero: ' + id_inmueble)\n",
    "    \n",
    "    url = \"https://www.idealista.com/inmueble/\" + id_inmueble + \"/\"\n",
    "    \n",
    "    browser.get(url)\n",
    "\n",
    "    html = browser.page_source\n",
    "    \n",
    "    soup = bs(html, 'lxml')\n",
    "\n",
    "    titulo = soup.find('span',{'class':'main-info__title-main'}).text\n",
    "    \n",
    "    print('\\n Titulo: ' + titulo)\n",
    "    \n",
    "    localizacion = soup.find('span',{'class':'main-info__title-minor'}).text.split(',')[0]\n",
    "\n",
    "    print('\\n Localizacion: ' + localizacion)\n",
    "    precio = int(soup.find('span',{'class':'txt-bold'}).text.replace('.',''))\n",
    "\n",
    "    c1 = soup.find('div',{'class':'details-property'}).find('div',{'class': 'details-property-feature-one'})\n",
    "\n",
    "    caract_basicas = [caract.text.strip() for caract in c1.find_all('li')]\n",
    "    \n",
    "    #print('Caracteristicas basicas:' + caract_basicas)\n",
    "\n",
    "    c2 = soup.find('div',{'class':'details-property'}).find('div',{'class': 'details-property-feature-two'})\n",
    "\n",
    "    caract_extra = [caract.text.strip() for caract in c2.find_all('li')]\n",
    "    \n",
    "    #print('Caracteristicas extras:' + caract_extra)\n",
    "    \n",
    "    casas['titulo'] = titulo\n",
    "    \n",
    "    casas['localizacion'] = localizacion\n",
    "    \n",
    "    casas['precio'] = precio\n",
    "    \n",
    "    casas['caracteristicas_basicas'] = caract_basicas\n",
    "    \n",
    "    casas['caracteristicas_extras'] = caract_extra\n",
    "    \n",
    "    df_casas = pd.DataFrame(casas)\n",
    "    \n",
    "    return(df_casas.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Casa numero: 104440887\n",
      "\n",
      " Titulo: Piso en venta en paseo del General Martínez Campos\n",
      "\n",
      " Localizacion: Almagro\n",
      "\n",
      " Casa numero: 103666203\n",
      "\n",
      " Titulo: Piso en venta en avenida de América, 37\n",
      "\n",
      " Localizacion: Prosperidad\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     df_casas_completas \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_casas_completas, df_inmueble], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Espera un tiempo aleatorio entre 4 y 8 segundos para simular comportamiento humano\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Al finalizar, df_casas_completas tendrá los datos de todos los inmuebles\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_casas_completas)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Inicializa el DataFrame vacio donde almacenaremos los datos de todos los inmuebles\n",
    "df_casas_completas = pd.DataFrame()\n",
    "\n",
    "# Itera sobre cada ID en el DataFrame df_casas\n",
    "for i in range(len(df_casas)):\n",
    "    # Obtén el ID de la fila actual\n",
    "    id_inmueble = df_casas.iloc[i]['ID']\n",
    "    \n",
    "    # Llama a la función parsear_inmueble con el ID\n",
    "    df_inmueble = parsear_inmueble(id_inmueble)\n",
    "    \n",
    "    # Concatena el DataFrame actual con el DataFrame de los nuevos datos\n",
    "    df_casas_completas = pd.concat([df_casas_completas, df_inmueble], ignore_index=True)\n",
    "    \n",
    "    # Espera un tiempo aleatorio entre 4 y 8 segundos para simular comportamiento humano\n",
    "    time.sleep(random.randint(4, 8))\n",
    "\n",
    "# Al finalizar, df_casas_completas tendrá los datos de todos los inmuebles\n",
    "print(df_casas_completas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_casas_completas.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y exportamos todos los datos a otro csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_casas_completas.to_csv('casas_idealista.csv', index = False, sep = ';', encoding = 'utf-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Carga de datos DATAMAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datos de centros sanitarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carme\\AppData\\Local\\Temp\\ipykernel_11968\\87057601.py:41: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  lon, lat = transform(utm_proj, wgs84_proj, utm_x, utm_y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'datos_madrid_latlon.csv' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import csv\n",
    "import requests\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "# URL del archivo CSV\n",
    "base_url = \"https://datos.comunidad.madrid/catalogo/dataset/d8a0a444-adf5-4c04-8999-0eac3de52cb7/resource/2948b4da-8b39-42b7-b667-779a5284f39d/download/centros_servicios_establecimientos_sanitarios.csv\"\n",
    "\n",
    "# Realizamos la petición\n",
    "result = requests.get(base_url)\n",
    "\n",
    "# Si la conexión es exitosa, procesamos el CSV\n",
    "if result.status_code == 200:\n",
    "    # Convertimos el texto del CSV en una lista de diccionarios, con ';' como delimitador\n",
    "    csv_data = result.text.splitlines()\n",
    "    reader = csv.DictReader(csv_data, delimiter=';')\n",
    "\n",
    "    # Filtramos solo los datos del municipio de Madrid y eliminamos duplicados por 'centro_nro_registro'\n",
    "    seen_centers = set()\n",
    "    filtered_data = []\n",
    "    for row in reader:\n",
    "        centro_nro_registro = row.get(\"centro_nro_registro\")\n",
    "        if row.get(\"municipio_nombre\") and row[\"municipio_nombre\"].lower() == \"madrid\" and centro_nro_registro not in seen_centers:\n",
    "            seen_centers.add(centro_nro_registro)  # Agregamos el centro para evitar duplicados\n",
    "            filtered_data.append(row)\n",
    "\n",
    "    # Configuramos el sistema de coordenadas UTM y WGS84 (latitud/longitud)\n",
    "    utm_proj = Proj(proj='utm', zone=30, ellps='WGS84')  # UTM Zone 30T (España)\n",
    "    wgs84_proj = Proj(proj='latlong', datum='WGS84')\n",
    "\n",
    "    # Convertimos las coordenadas UTM a latitud y longitud\n",
    "    for row in filtered_data:\n",
    "        try:\n",
    "            # Verificamos si las coordenadas UTM están presentes y no están vacías\n",
    "            if row[\"localizacion_coordenada_x\"] and row[\"localizacion_coordenada_y\"]:\n",
    "                # Convertimos las coordenadas UTM a float\n",
    "                utm_x = float(row[\"localizacion_coordenada_x\"])\n",
    "                utm_y = float(row[\"localizacion_coordenada_y\"])\n",
    "                \n",
    "                # Convertimos a latitud y longitud\n",
    "                lon, lat = transform(utm_proj, wgs84_proj, utm_x, utm_y)\n",
    "                \n",
    "                # Añadimos las nuevas coordenadas al diccionario\n",
    "                row[\"latitud\"] = lat\n",
    "                row[\"longitud\"] = lon\n",
    "            else:\n",
    "                # Si las coordenadas están vacías, asignamos None o dejamos en blanco\n",
    "                row[\"latitud\"] = None\n",
    "                row[\"longitud\"] = None\n",
    "\n",
    "            # Eliminamos la columna 'oferta_asistencial' si existe\n",
    "            row.pop(\"oferta_asistencial\", None)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir coordenadas para la fila: {row['centro_nro_registro']}\", e)\n",
    "\n",
    "    # Guardamos los datos filtrados y transformados en un nuevo archivo CSV\n",
    "    with open(\"datos_madrid_latlon.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        # Revisamos si 'latitud' y 'longitud' ya están en las claves y los agregamos solo si no están\n",
    "        fieldnames = list(filtered_data[0].keys())\n",
    "        if \"latitud\" not in fieldnames:\n",
    "            fieldnames.append(\"latitud\")\n",
    "        if \"longitud\" not in fieldnames:\n",
    "            fieldnames.append(\"longitud\")\n",
    "\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(filtered_data)\n",
    "    \n",
    "    print(\"Archivo 'datos_madrid_latlon.csv' generado exitosamente.\")\n",
    "else:\n",
    "    print(\"Error en la solicitud:\", result.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limpieza de datos y coordenadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procedemos al tratamiento de los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              titulo localizacion   precio  \\\n",
      "0          Piso en venta en calle del Duque de Sesto         Goya  1290000   \n",
      "1          Piso en venta en calle del Duque de Sesto         Goya  1290000   \n",
      "2                          Ático en venta en Adelfas       Retiro   880000   \n",
      "3   Chalet pareado en venta en calle Virgen de lo...      Aravaca  2280000   \n",
      "4   Casa o chalet independiente en venta en calle...   Valdemarín  3500000   \n",
      "\n",
      "                             caracteristicas_basicas  \\\n",
      "0  [133 m² construidos, 2 habitaciones, 3 baños, ...   \n",
      "1  [133 m² construidos, 2 habitaciones, 3 baños, ...   \n",
      "2  [175 m² construidos, 4 habitaciones, 3 baños, ...   \n",
      "3  [Chalet pareado, 505 m² construidos, 7 habitac...   \n",
      "4  [Casa o chalet independiente, 749 m² construid...   \n",
      "\n",
      "                              caracteristicas_extras  numero_calle  \\\n",
      "0   ['Aire acondicionado', 'Consumo:', 'Emisiones:']           NaN   \n",
      "1   ['Aire acondicionado', 'Consumo:', 'Emisiones:']           NaN   \n",
      "2  ['Aire acondicionado', 'Piscina', 'Zonas verde...           NaN   \n",
      "3  ['Aire acondicionado', 'Piscina', 'Jardín', 'E...           NaN   \n",
      "4  ['Aire acondicionado', 'Piscina', 'Jardín', 'E...           NaN   \n",
      "\n",
      "   metros_cuadrados  habitaciones  baños  parcela_m2 plaza_garaje  \\\n",
      "0               133           2.0    3.0         NaN         None   \n",
      "1               133           2.0    3.0         NaN         None   \n",
      "2               175           4.0    3.0         NaN         True   \n",
      "3               505           7.0    5.0      1127.0         True   \n",
      "4               749           7.0   10.0      2987.0         True   \n",
      "\n",
      "                     estado orientacion  año_construccion  calefaccion  \\\n",
      "0  Segunda mano/buen estado         sur            1941.0          Gas   \n",
      "1  Segunda mano/buen estado         sur            1941.0          Gas   \n",
      "2  Segunda mano/buen estado        None               NaN  Gas natural   \n",
      "3  Segunda mano/buen estado        None            1997.0         None   \n",
      "4  Segunda mano/buen estado        None            2003.0         None   \n",
      "\n",
      "               planta ascensor  \n",
      "0  Planta 1ª exterior     True  \n",
      "1  Planta 1ª exterior     True  \n",
      "2  Planta 6ª exterior     True  \n",
      "3                None     None  \n",
      "4                None     None  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv(\"casas_idealista.csv\", sep=\";\", encoding=\"utf-16\")\n",
    "\n",
    "# Convertir 'caracteristicas_basicas' en una lista de Python, si no está ya en ese formato\n",
    "df['caracteristicas_basicas'] = df['caracteristicas_basicas'].apply(eval)\n",
    "\n",
    "# Función para extraer el número de calle\n",
    "def extraer_numero_calle(titulo):\n",
    "    # Busca un número en la dirección que esté seguido de coma o espacio\n",
    "    match = re.search(r',\\s?(\\d+)', titulo)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Aplicar la función para crear la columna 'numero_calle'\n",
    "df['numero_calle'] = df['titulo'].apply(extraer_numero_calle)\n",
    "\n",
    "# Función para extraer los datos de 'caracteristicas_basicas'\n",
    "def extraer_datos(lista):\n",
    "    datos = {\n",
    "        'metros_cuadrados': None,\n",
    "        'habitaciones': None,\n",
    "        'baños': None,\n",
    "        'parcela_m2': None,\n",
    "        'plaza_garaje': None,\n",
    "        'estado': None,\n",
    "        'orientacion': None,\n",
    "        'año_construccion': None,\n",
    "        'calefaccion': None,\n",
    "        'planta': None,\n",
    "        'ascensor': None,\n",
    "    }\n",
    "\n",
    "    for item in lista:\n",
    "        # Solo procesa elementos que contengan características específicas de la vivienda\n",
    "        if re.search(r'\\bm²\\b|\\bhabitaciones\\b|\\bbaños\\b|garaje|estado|Orientación|Construido en|Calefacción|Planta|ascensor|Parcela', item):\n",
    "            if re.search(r'\\d+\\s?m² construidos', item):\n",
    "                datos['metros_cuadrados'] = int(re.search(r'\\d+', item).group())\n",
    "            elif 'habitaciones' in item:\n",
    "                datos['habitaciones'] = int(re.search(r'\\d+', item).group())\n",
    "            elif 'baños' in item:\n",
    "                datos['baños'] = int(re.search(r'\\d+', item).group())\n",
    "            elif 'Parcela de' in item:\n",
    "                datos['parcela_m2'] = int(re.search(r'\\d+', item.replace('.', '')).group())\n",
    "            elif 'garaje' in item:\n",
    "                datos['plaza_garaje'] = True\n",
    "            elif 'Segunda mano' in item or 'para reformar' in item:\n",
    "                datos['estado'] = item\n",
    "            elif 'Orientación' in item:\n",
    "                datos['orientacion'] = item.split(' ')[-1]\n",
    "            elif 'Construido en' in item:\n",
    "                datos['año_construccion'] = int(re.search(r'\\d+', item).group())\n",
    "            elif 'Calefacción' in item:\n",
    "                datos['calefaccion'] = item.split(': ')[-1]\n",
    "            elif 'Planta' in item:\n",
    "                datos['planta'] = item\n",
    "            elif 'ascensor' in item:\n",
    "                datos['ascensor'] = True\n",
    "\n",
    "    return datos\n",
    "\n",
    "# Aplicar la función y expandir los resultados en el DataFrame\n",
    "datos_df = df['caracteristicas_basicas'].apply(extraer_datos).apply(pd.Series)\n",
    "\n",
    "# Combinar los datos originales con las nuevas columnas\n",
    "df = pd.concat([df, datos_df], axis=1)\n",
    "\n",
    "# Guardar el DataFrame en un nuevo archivo CSV si se desea\n",
    "df.to_csv('casas_completo.csv', index=False, sep=';')\n",
    "# Guardar el DataFrame en un nuevo archivo CSV con punto y coma como delimitador\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar el DataFrame final\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos las coordenadas a las direcciones utilizando DireccionesVigentes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'casas_completo_actualizado.csv' creado con éxito.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Cargar los datos\n",
    "direcciones_df = pd.read_csv(\"DireccionesVigentes.csv\", encoding=\"latin1\", delimiter=\";\")\n",
    "casas_df = pd.read_csv(\"casas_completo.csv\", encoding=\"utf-8\", delimiter=\";\")\n",
    "\n",
    "# Limpiar y normalizar nombres de calles en DireccionesVigentes\n",
    "direcciones_df[\"VIA_NOMBRE\"] = direcciones_df[\"VIA_NOMBRE\"].str.upper().str.strip()\n",
    "\n",
    "# Eliminar duplicados en direcciones_df para que solo haya un valor de coordenadas por calle\n",
    "direcciones_df = direcciones_df.drop_duplicates(subset=[\"VIA_NOMBRE\"], keep=\"first\")\n",
    "\n",
    "# Función para extraer y normalizar el nombre de la calle del título\n",
    "def extraer_nombre_calle(titulo):\n",
    "    # Buscar el tipo de vía seguido de la última palabra del nombre de la calle\n",
    "    match = re.search(r'\\b(CALLE|AVENIDA|PASEO|PLAZA|RONDA|CAMINO)\\s+(?:DE(L|LA)?\\s+)?(\\w+)$', titulo, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(3).upper().strip()  # Solo capturar la última palabra\n",
    "    return None\n",
    "\n",
    "# Crear una columna temporal en casas_df para almacenar el nombre de la calle extraído\n",
    "casas_df[\"CALLE_EXTRAIDA\"] = casas_df[\"titulo\"].apply(extraer_nombre_calle)\n",
    "\n",
    "# Hacer el merge de ambas bases de datos en función del nombre de la calle\n",
    "casas_completo = casas_df.merge(direcciones_df[['VIA_NOMBRE', 'LATITUD', 'LONGITUD']],\n",
    "                                left_on=\"CALLE_EXTRAIDA\", right_on=\"VIA_NOMBRE\", how=\"left\")\n",
    "\n",
    "# Eliminar la columna temporal y renombrar columnas\n",
    "casas_completo.drop(columns=[\"CALLE_EXTRAIDA\", \"VIA_NOMBRE\"], inplace=True)\n",
    "\n",
    "# Guardar el archivo actualizado\n",
    "casas_completo.to_csv(\"casas_completo_actualizado.csv\", index=False)\n",
    "print(\"Archivo 'casas_completo_actualizado.csv' creado con éxito.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí eliminamos todos aquellos pisos de los cuales no ha sido posible obtener las coordenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'casas_con_coordenadas.csv' creado con éxito.\n"
     ]
    }
   ],
   "source": [
    "casas_con_coordenadas = casas_completo.dropna(subset=['LATITUD', 'LONGITUD'])\n",
    "\n",
    "# Guardar el archivo de casas con coordenadas\n",
    "casas_con_coordenadas.to_csv(\"casas_con_coordenadas.csv\", index=False)\n",
    "print(\"Archivo 'casas_con_coordenadas.csv' creado con éxito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles en el DataFrame: ['titulo', 'localizacion', 'precio', 'numero_calle', 'metros_cuadrados', 'habitaciones', 'baños', 'parcela_m2', 'plaza_garaje', 'estado', 'orientacion', 'año_construccion', 'calefaccion', 'planta', 'ascensor', 'LATITUD', 'LONGITUD']\n",
      "Archivo 'casas_con_coordenadas_transformadas.csv' creado con éxito.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Definir una función para convertir coordenadas de DMS a decimal\n",
    "def dms_a_decimal(dms):\n",
    "    # Separar grados, minutos, segundos y dirección\n",
    "    degrees, minutes, seconds, direction = re.split(\"[°'\\\" ]+\", dms.strip())\n",
    "    decimal = float(degrees) + float(minutes) / 60 + float(seconds) / 3600\n",
    "    if direction in ['S', 'W']:\n",
    "        decimal = -decimal\n",
    "    return decimal\n",
    "\n",
    "# Cargar los datos desde un CSV\n",
    "casas_df = pd.read_csv(\"casas_con_coordenadas.csv\", encoding=\"utf-8\", delimiter=\",\")\n",
    "\n",
    "# Imprimir los nombres de las columnas para depuración\n",
    "print(\"Columnas disponibles en el DataFrame:\", casas_df.columns.tolist())\n",
    "\n",
    "# Convertir las coordenadas de LATITUD y LONGITUD a decimal\n",
    "# Asegúrate de que los nombres coincidan exactamente con los nombres de columna\n",
    "if 'LATITUD' in casas_df.columns and 'LONGITUD' in casas_df.columns:\n",
    "    casas_df['LATITUD'] = casas_df['LATITUD'].apply(dms_a_decimal)\n",
    "    casas_df['LONGITUD'] = casas_df['LONGITUD'].apply(dms_a_decimal)\n",
    "else:\n",
    "    print(\"No se encontraron las columnas 'LATITUD' y/o 'LONGITUD' en el DataFrame.\")\n",
    "\n",
    "# Guardar el nuevo archivo CSV con las coordenadas transformadas\n",
    "casas_df.to_csv(\"casas_con_coordenadas_transformadas.csv\", index=False, sep=';')\n",
    "print(\"Archivo 'casas_con_coordenadas_transformadas.csv' creado con éxito.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Aplicacion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
